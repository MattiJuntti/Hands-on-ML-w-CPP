# 01: Introduction to Machine Learning witg C++

## Introduction

Different approaches to make computers solve tasks;

$\rightarrow$ Defining explicit algorithms
<br>$\rightarrow$ Definin implicit algorithms via <span style = "color:#42a5f5"><b>mathematical methods</b></span>.

<span style = "color:#42a5f5"><b>Machine learning</b></span> (ML)
is one the methods.

ML helps solving tasks in many activities. E.g

- Customer relationship management
- Business Intelligence
- Human resource

C++ libraries for ML are, for example, `Eigen`, `xtensor`,
`Shark-ML`, `Shogun`, and `Dlib`.

## Understanding the fundamentals of ML

$\rightarrow$ Many approaches to create and train ML models.
<br>$\rightarrow$ Apart from the approach, <span style = "color:#42a5f5"><b>parameters</b></span> (params) affect how to train a model
<br>$\rightarrow$ Chosen technique effects also a chosen ML model.

Usually, chosen training technique uses some <span style =
"color:#42a5f5"><b>numerical optimization algorithm</b></span> that
finds the minimal value of some <span style =
"color:#42a5f5"><b>target function</b></span>.

$\rightarrow$ The target function is usually called <span style = "color:#42a5f5"><b>loss function</b></span>.

Loss function is used to penalize the training algorithm to
minimize errors.

## Venturing into the techniques of ML

ML can be divided into two approaches

1. <span style = "color:#42a5f5"><b>Supervised learning</b></span>:
   An approach based on the use of labeled data.

$\rightarrow$ Labeled data is _a set of known data samples with known target outputs_.

2. <span style = "color:#42a5f5"><b>Unsupervised learning</b></span>: An approach not needing labeled data.

### Supervised learning

Supervised learning can be split into two parts:

1. <span style = "color:#42a5f5"><b>Classification
models</b></span>: Predict infinite and distinct types of
categories.

$\rightarrow$ Applied in speech and text regocnition, object identification on images, credit scoring, and others.
<br>$\rightarrow$ Typical algorithms for creating classification models are;

- <span style = "color:#42a5f5"><b>Support vector machine</b></span> (SVM),
- <span style = "color:#42a5f5"><b>decision tree</b></span> approaches,
- <span style = "color:#42a5f5"><b>k-nearest neighbours</b></span> (KNN),
- <span style = "color:#42a5f5"><b>logistic regression</b></span>,
- <span style = "color:#42a5f5"><b>naive Bayes</b></span>, and
- <span style = "color:#42a5f5"><b>neural networks</b></span>.

2. <span style = "color:#42a5f5"><b>Regression models</b></span>: Predict continuous responses.

$\rightarrow$ Application examples; algorithmic trading, load forecasting, revenue prediction.

Regression models usually make sense if the output of the given
labeled data is real numbers.

Typical algorithms for regression models are

- <span style = "color:#42a5f5"><b>linear</b></span> and <span style = "color:#42a5f5"><b>multivariate regressions</b></span>,
- <span style = "color:#42a5f5"><b>polynomial regression models</b></span>,
- <span style = "color:#42a5f5"><b>stepwise regressions</b></span>.

### Unsupervised learning

Learning algorithms that do not use labeled datasets.

$\rightarrow$ Tries to use relations in data to find hidden
patterns for making predictions.

Well-known unsupervised learning technique is called <span style =
"color:#42a5f5"><b>clustering</b></span>; Means divising a given
set of data in a limited number of groups according to properties
of data items.

Applications: Market researches, exploratory analysis,
deoxyribonucleic acis (DNA) analysis, image segmentation, and
object detection.

Typical algorithms are:

- <span style = "color:#42a5f5"><b>k-means</b></span>,
- <span style = "color:#42a5f5"><b>k-medoids</b></span>,
- <span style = "color:#42a5f5"><b>Gaussian mixture models</b></span>,
- <span style = "color:#42a5f5"><b>hierarchical clustering</b></span>, and
- <span style = "color:#42a5f5"><b>hidden Markov models</b></span>.

##  Dealing wiht ML models

ML models can be interpreted as functions that take different types
of parameters and provide outputs for given inputs. Developers
configure the behavior of ML models for solving problems by
adjusting model parameters.

$\rightarrow$ It's possible to think ML as a process as finding suitable combination of parameters.

ML model's parameters can be split into two types.

1. Parameters _internal_ to a model.

$\rightarrow$ These parameters can be estimated from a training data.

2. Paremeters _external_ to model.

$\rightarrow$ Are called <span style = "color:#42a5f5"><b>hyperparameters</b></span>.
<br>$\rightarrow$ Cannot be estimated from data.

Internal parameters are characterized by following properties

- Necessary for making predictions.
- Define the quality of a model on a given problem.
- Can be learned from a training data.
- Are usually part of a model.

$\rightarrow$ If model contains fixed number of internal parameters, model is called <span style = "color:#42a5f5"><b>parametric</b></span>.
<br>$\rightarrow$ <span style = "color:#42a5f5"><b>Non-parametric</b></span> otherwise.

Examples of internal parameters are:

- Weights of <span style = "color:#42a5f5"><b>artificial neural networks</b></span> (ANN).
- Support vector values for SVM models.
- Polynomial coefficients for linear regression or logistic regression.

Hyperparameter have following characteristics:

- Used to configure algorithms that estimate model parameters.
- Usually defined by a practicioner.
- Estimation is often based on using heuristics.
- Specific to a modeling problem.

$\rightarrow$ Hyperparameters can be hard to estimate, and need micro-management.

Some heuristics are

- Rules of thumb,
- copying values from similar projects,
- grid search for hyperparameters.

Examples of hyperparameters are:

- C and sigma parameters used in a SVM algorithm for classification quality configuration.
- Learning rate params used in neural network training process for configuring algorithm convergence.
- The $k$ value used in KNN algorithm for configuring a number of neighbours.

## Model parameter estimation

Usually uses some optimiazation algorithm, and are used via
algorithms based on the optimization of a loss function.

$\rightarrow$ Function that evaluates how well model predicts on the data is called <span style = "color:#42a5f5"><b>loss function</b></span>.

If predicions are very different from target outputs, the loss
function will return a value that is interpreted as a bad one.

$\rightarrow$ Is usually a large number.
<br>$\rightarrow$ In such a way, a loss function penalizes an optimization algorithm when it moves in the wrong direction.
<br>$\rightarrow$ Hence the general idea is to _minimize the value of loss function to reduce penalties_.
<br>$\rightarrow$ No universal loss function for optimization algorithms.

Different factors deterine how to choose a loss function. Examples
of such functions are:

- Specifics of a given problem,
- ease of calculating derivates, and
- percentage of outliers in the dataset.

Term <span style = "color:#42a5f5"><b>optimizer</b></span> is used
to define an algorithm that connects a loss function and a
technique for updating model parameters in response to the values
of the loss funcion.

Some optimizer examples:

- <span style = "color:#42a5f5"><b>Gradient descent</b></span>,
- <span style = "color:#42a5f5"><b>adagrad</b></span>,
- <span style = "color:#42a5f5"><b>RMSProp</b></span>,
- <span style = "color:#42a5f5"><b>Adam</b></span>,
- and others.

$\rightarrow$ Developign optimizers is an active area of research.

## An overview of linear algebra

Development of GPUs furthered development of ML. Example libraries
used for GPU computing are `Cuda`, `OpenCL`, and `cuBLAS`.
Processor architectures have specialized instruction sets for
vectorized computing; `AVx`, `SSE`, and `MMx`.

$\rightarrow$ Called _single instructions multiple data_ (SIMD) inst. sets.

Also there's linear algebra libraries such as `Eigen`, `xtensor`,
`VienaCL`, and others.

## Learning the concepts of linear algebra

Linear algebra studies objects of a linear nature: Vector (or
linear) spaces, linear representatioins, and systems of linear
equations. Main ideas of linear algebra are determinants, matrices,
conjuation, and tensor calculus.

## Basic linear algebra operations

#### Element-wise operations

Performed in an element-wise manner on vectors, matrices, or
tensors of the same size.

#### Dot product

Deals with two equal-length series of numbers and returns a single
number. A dot product example is:

$$
    \mathbf{C}_{i,j} = \sum_{k = 1}^m A_{i,k}B_{k,}
$$

where $i = 1, \ldots, n$ and $j = 1, \ldots, p$.

#### Transposing

$$
    ( \mathbf{A} )^\top_{i,j} = \mathbf{A}_{j,i}
$$

#### Norm

This operation results in a size of a vector;

$$
    ||x||_p = \left( \sum_i |x_i|^p \right)^{ \frac{1}{p}  }
$$

Generic name is $L^p$ norm for $p \in \mathbb{R}, p \geq <$.

$\rightarrow$ When $p = 2$, the norm is known as _Euclidean norm_.

Another well known norm is the <span style =
"color:#42a5f5"><b>squared $L^2$ norm</b></span>. Equation for the
norm is $x^\top x$.

$\rightarrow$ The squared $L^2$ norm is more suitable for mathematical and computational operations than the $L^2$ norm.

Each partial derivative of squared $L^2$ norm depends only on the
corresponding element of $x$, whereas the partial derivatives of
the $L^2$ norm depends on the entire vector.

$\rightarrow$ This property has a vital role in optimization algorithms.

#### Inversion

...

Matrices and tensors can be used to define training datasets for
training, etc. Element-wise operations can be used to manipulate
dataset, such as scaling. Linear algebra operations allows also for
appplying many different transformations on dataset. Dot-product
can be used to update model parameters etc.

## Tensor representation in computing

Tensors can be represented in computer's memory in many different
ways. For example as a linear array.

$\rightarrow$ Either row-major or column-major ordering.
<br>$\rightarrow$ Data layouts have a significant impact on computational performance.
<br>$\rightarrow$ Cache misses etc.
<br>$\rightarrow$ Contiguous data allows for use of SIMD-operations.

## Linear algebra API samples

### Eigen

`Eigen` is a general-purpose linear algebra C++ library. In
`Eigen`, all matrices and vectors are objects of the `Matrix`
template class.

$\rightarrow$ Tensor are not present in the official API, but exist as a submodules.

Define a type for a matrix with known dimensions and floating-point data type:

```c++
typedef Eigen::Matrix<float, 3, 3> MyMatrix33f;
```

and a vector by typing

```c++
typedef Eigen::Matrix<float, 3, 1> MyVector3f;
```

`Eigen` already has many predefined types for vector and matrix
objects, `Eigen::Matrix3f` (floating-point $3 \times 3$ matrix
type) or `Eigen::RowVector2f` (floating-point $1 \times 2$ matrix).

$\rightarrow$ `Eigen` is not limited to static sized matrices.

Run-time defined matrix can be defined as,

```c++
typedef Eigen::Matrix<double, Eigen::Dynamic, Eigen::Dynamic> MyMatrix;
```

The above type definitions can now be used as

```c++
MyMatrix33f a;
MyVector3f v;
MyMatrix m(10,15);
```

for example. The variables `a`, `v`, and `m` can be defined as

```c++
a = MyMatrix33f::Zero();
a = MyMatrix33f::Identity();
v = MyVector3f::Random();
```

Matrices can be seeded with values by using the <span style =
"color:#42a5f5"><b>comma-initializer</b></span> syntax:

```c++
a << 1, 2, 3,
     4, 5, 6,
     7, 8, 9;
```

so the matrix `a` would have the values

$$
\begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6 \\
    7 & 8 & 9
\end{bmatrix}
$$

as a result.

Direc element access in `Eigen` happens with the `()` operator:

```c++
a(0,0) = 3;
```

`Eigen`'s `Map` type can be used to wrap an existent C++ array or
vector in the `Matrix` type object:

```c++
int data[] = {1, 2, 3, 4};
Eigen::Map<Eigen::RowVectorxi> v (data, 4);
std::vector<float> data = {1, 2, 3, 4, 5, 6, 7, 8, 9};
Eigen::Map<MyMatrix33f> a(data.data());
```

Initialized matrices can be used in mathematical operations. For
example,

```c++
using namespace Eigen;

auto a = Matrix2d::Random();
auto b = Matrix2d::Random();
auto result = a + b;

result = a.array() * b.array(); // element wise multiplication
result = a.array() / b.array();
a += b;
result = a * b; // matrix multiplication
a = b.array() * 4;
```

In `Eigen`, arithmetic operators such as `operator+` do not perform
any computation by themselves. These operations return a <span
style = "color:#42a5f5"><b>expression object</b></span>, which
describe what computation to perform.

Sometimes some computation is needed to be done only part of a
block. `Eigen` allows this with the `block` method, which needs
four parameters:

- `i` and `j` that define a starting point, and
- `p` and `q` that define the size of a block.

Example using the `block` methods is:

```c++
Eigen::Matrixxf m(4,4);
Eigen::Matrix2f b = m.block(1,1,2,2);
m.block(1,1,2,2) *= 4;
```

As many other linear algebra libraries, `Eigen` too supports <span
style = "color:#42a5f5"><b>broadcasting</b></span> with the
`colwise` and `rowwise` methods. For example, the following code

```c++
Eigen::Matrixxf mat(2,4);
Eigen::Vectorxf v(2); // column vector
mat.colwise() += v;
```

is equivalent to result

$$
    \begin{bmatrix}
        1 & 2 & 3 \\
        4 & 5 & 6
    \end{bmatrix}.\operatorname{colwise()}
    \ + \ 
    \begin{bmatrix}
        0 \\ 2
    \end{bmatrix}
    \ = \ 
    \begin{bmatrix}
        1 & 2 & 3 \\
        \mathbf{5} & \mathbf{6} & \mathbf{7}
    \end{bmatrix}.
$$

### Using `xtensor`

The `xtensor` library is a C++ library for numerical analysis with
multidimensional array expressions. Containers of `xtensor` are
isnpired by NumPy, the Python array programming library.jo

The `xtensor`'s `xarray` type is a dynamically sized
multidimensional array:

```c++
std::vector<size_t> shape = { 3, 2, 4 };
xt::array<double, xt::layout_type::row_major> a (shape);
```

The `xtensor` type is a multidimensional array whose dimensions are
fixed at _compilation time_. Exact dimension values can be
configured in the initialization step;

```c++
std::array<size_t, 3> shape = { 3, 2, 4 };
xt::tensor<double, 3> a(shape);
```

The `xtensor_fixed` type is a multidimensional array with a
dimension shape fixed at compile time, as shown in the following
code snippet:

```c++
xt::tensor_fixed<double, xt::shape<3, 2, 4>> a;
```

The `xtensor` library also implements arithmetic operators with
expression template techniques such as `Eigen`.

$\rightarrow$ The computation happens lazily, and the actual result is calculated when the whole expression is evaluated.
<br>$\rightarrow$ Container definitions are also expressions.

Different kinds of container initialization in the `xtensor` library.

Example:

- `xtensor` array initialized with _initializer lists_:

```c++
xt::xarray <double> arr1 {
    {1.0, 2.0, 3.0},
    {2.0, 5.0, 7.0},
    {2.0, 5.0, 7.0}
};
```

The `xtensor` library has builder functions for special tensor
types. Examples;

```c++
std::vector <uint64_t> shape = {2, 2};
xt::ones(shape);
xt::zero(shape);
xt::eye(shape);
```

C++ arrays can be mapped to `xtensor` container with the
`xt::adapt` function. The function `xt::adapt` return the object
that uses the memory and values from the underlying object;

```c++
std::vector<float> data {1, 2, 3, 4};
std::vector<size_t> shape {2, 2};
auto data_x = xt::adapt(data, shape);
```

Direct access in `xtensor` container is done with the `operator()`;

```c++
std::vector<size_t> shape {3, 2, 4};
xt::array<float> a = xt::ones<float>(shape);
a(2,1,3) = 3.14f;
```

Partial access to the `xtensor` containers is done with `xt::view`
function:

```c++
xt::array<int> a {
    { 1,  2,  3,  4},
    { 5,  6,  7,  8},
    { 9, 10, 11, 12},
    {12, 14, 15, 16}
};

auto b = xt::view(a, xt::range(1, 3), xt::range(1, 3));
```

This results in a matrix view;

$$
    \begin{bmatrix}
        1  & 2  & 3  & 4  \\
        5  & 6  & 7  & 8  \\
        9  & 10 & 11 & 12 \\
        13 & 14 & 15 & 16
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
        6 &  7 \\
       10 & 11
    \end{bmatrix}
$$

$\rightarrow$ `xtensor` implements automatic broadcasting.
<br>$\rightarrow$ With two matrices of different sizes, `xtensor` broadcasts smaller dimension across the leading dimension of the other array.
<br>$\rightarrow$ Allows adding different sized matrices;

```c++
auto m = xt::random::rand<double>({2,2});
auto v = xt::random::rand<double>({2,1});
auto c = m + v;
```

### Using `Shark-ML`

Four container class for representing matrices and vectors in the
`Shark-ML` library.

$\rightarrow$ Linear algebra functionality is declared in the `remora` namespace.
<br>$\rightarrow$ Other functionality is in the `shark` namespace.

Example of a container class in `Shark-ML` library. The following
code creates a vector of size `100` filled with `1.0`.

$$
    remora::vector<double> b(100, 1.0);
$$

The `compressed_vector` type is a sparse array storing values in a
compressed format.

The `matrix` type is a dynamically sized dense matrix:

$$
    remora::matrix<double> C(2, 2);
$$

The `compressed_matrix` type is a sparse matrix storing values in a
compressed format.

$\rightarrow$ Two ways to initialize a matrix

1. Initializer list:

$$
    remora::matrix<float> m_ones {{1,1},{1,1}}; // 2 x 2 matrix
$$

2. Wrap existing C++ array into a container:

$$
    float data[] = {1, 2, 3, 4};
    remora::matrix<float> m(data, 2, 2);
    remora::vector<float> v(data, 4);
$$

$\rightarrow$ Also there's ofcourse the possibility of using direct access with the `operator()`.

`Shark-ML` allows for linear algebra.

Partially accessing data in `Shark-ML` containers can be done with:

- `subrange(x,i,j)`: Sub-vector of $x$ elements $\langle x_i, \ldots, x_{j-1} \rangle$.
- `subrange(A,i,j,k,l)`: Sub-matrix $A$ wiht elements $i, \ldots, j-1$ and $k, \ldots, l - 1$.
- `row(A,k)`: `k`th column of `A` as a vector proxy.
- `column(A,k)`: ...
- `rows(A,k,l)`: ...
- `columns(A,k,l)`: ...

There's no broadcasting implementation in the `Shark-ML` library,
but the library allows for reduction operations to be performed
independently on matrix rows or columns respectively:

```c++
remora::matrix<float> m{{1,2,3,4}, {5,6,7,8}};
auto cols = remora:as_columns(m);
remora::sum(cols);
```

How to add the same vector to each of the matrix columns:

```c++
remora::vector<float> v{10,10};
// Update matrix rows
for (size_t i = 0; i < m.size2(); ++i)
    remora::column(m,i) += v;
```
