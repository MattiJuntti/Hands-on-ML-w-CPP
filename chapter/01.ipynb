{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ed4756",
   "metadata": {},
   "source": [
    "# 01: Introduction to Machine Learning witg C++\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Different approaches to make computers solve tasks;\n",
    "\n",
    "$\\rightarrow$ Defining explicit algorithms\n",
    "<br>$\\rightarrow$ Definin implicit algorithms via <span style = \"color:#42a5f5\"><b>mathematical methods</b></span>.\n",
    "\n",
    "<span style = \"color:#42a5f5\"><b>Machine learning</b></span> (ML)\n",
    "is one the methods.\n",
    "\n",
    "ML helps solving tasks in many activities. E.g\n",
    "\n",
    "- Customer relationship management\n",
    "- Business Intelligence\n",
    "- Human resource\n",
    "\n",
    "C++ libraries for ML are, for example, `Eigen`, `xtensor`,\n",
    "`Shark-ML`, `Shogun`, and `Dlib`.\n",
    "\n",
    "## Understanding the fundamentals of ML\n",
    "\n",
    "$\\rightarrow$ Many approaches to create and train ML models.\n",
    "<br>$\\rightarrow$ Apart from the approach, <span style = \"color:#42a5f5\"><b>parameters</b></span> (params) affect how to train a model\n",
    "<br>$\\rightarrow$ Chosen technique effects also a chosen ML model.\n",
    "\n",
    "Usually, chosen training technique uses some <span style =\n",
    "\"color:#42a5f5\"><b>numerical optimization algorithm</b></span> that\n",
    "finds the minimal value of some <span style =\n",
    "\"color:#42a5f5\"><b>target function</b></span>.\n",
    "\n",
    "$\\rightarrow$ The target function is usually called <span style = \"color:#42a5f5\"><b>loss function</b></span>.\n",
    "\n",
    "Loss function is used to penalize the training algorithm to\n",
    "minimize errors.\n",
    "\n",
    "## Venturing into the techniques of ML\n",
    "\n",
    "ML can be divided into two approaches\n",
    "\n",
    "1. <span style = \"color:#42a5f5\"><b>Supervised learning</b></span>:\n",
    "   An approach based on the use of labeled data.\n",
    "\n",
    "$\\rightarrow$ Labeled data is _a set of known data samples with known target outputs_.\n",
    "\n",
    "2. <span style = \"color:#42a5f5\"><b>Unsupervised learning</b></span>: An approach not needing labeled data.\n",
    "\n",
    "### Supervised learning\n",
    "\n",
    "Supervised learning can be split into two parts:\n",
    "\n",
    "1. <span style = \"color:#42a5f5\"><b>Classification\n",
    "models</b></span>: Predict infinite and distinct types of\n",
    "categories.\n",
    "\n",
    "$\\rightarrow$ Applied in speech and text regocnition, object identification on images, credit scoring, and others.\n",
    "<br>$\\rightarrow$ Typical algorithms for creating classification models are;\n",
    "\n",
    "- <span style = \"color:#42a5f5\"><b>Support vector machine</b></span> (SVM),\n",
    "- <span style = \"color:#42a5f5\"><b>decision tree</b></span> approaches,\n",
    "- <span style = \"color:#42a5f5\"><b>k-nearest neighbours</b></span> (KNN),\n",
    "- <span style = \"color:#42a5f5\"><b>logistic regression</b></span>,\n",
    "- <span style = \"color:#42a5f5\"><b>naive Bayes</b></span>, and\n",
    "- <span style = \"color:#42a5f5\"><b>neural networks</b></span>.\n",
    "\n",
    "2. <span style = \"color:#42a5f5\"><b>Regression models</b></span>: Predict continuous responses.\n",
    "\n",
    "$\\rightarrow$ Application examples; algorithmic trading, load forecasting, revenue prediction.\n",
    "\n",
    "Regression models usually make sense if the output of the given\n",
    "labeled data is real numbers.\n",
    "\n",
    "Typical algorithms for regression models are\n",
    "\n",
    "- <span style = \"color:#42a5f5\"><b>linear</b></span> and <span style = \"color:#42a5f5\"><b>multivariate regressions</b></span>,\n",
    "- <span style = \"color:#42a5f5\"><b>polynomial regression models</b></span>,\n",
    "- <span style = \"color:#42a5f5\"><b>stepwise regressions</b></span>.\n",
    "\n",
    "### Unsupervised learning\n",
    "\n",
    "Learning algorithms that do not use labeled datasets.\n",
    "\n",
    "$\\rightarrow$ Tries to use relations in data to find hidden\n",
    "patterns for making predictions.\n",
    "\n",
    "Well-known unsupervised learning technique is called <span style =\n",
    "\"color:#42a5f5\"><b>clustering</b></span>; Means divising a given\n",
    "set of data in a limited number of groups according to properties\n",
    "of data items.\n",
    "\n",
    "Applications: Market researches, exploratory analysis,\n",
    "deoxyribonucleic acis (DNA) analysis, image segmentation, and\n",
    "object detection.\n",
    "\n",
    "Typical algorithms are:\n",
    "\n",
    "- <span style = \"color:#42a5f5\"><b>k-means</b></span>,\n",
    "- <span style = \"color:#42a5f5\"><b>k-medoids</b></span>,\n",
    "- <span style = \"color:#42a5f5\"><b>Gaussian mixture models</b></span>,\n",
    "- <span style = \"color:#42a5f5\"><b>hierarchical clustering</b></span>, and\n",
    "- <span style = \"color:#42a5f5\"><b>hidden Markov models</b></span>.\n",
    "\n",
    "##  Dealing wiht ML models\n",
    "\n",
    "ML models can be interpreted as functions that take different types\n",
    "of parameters and provide outputs for given inputs. Developers\n",
    "configure the behavior of ML models for solving problems by\n",
    "adjusting model parameters.\n",
    "\n",
    "$\\rightarrow$ It's possible to think ML as a process as finding suitable combination of parameters.\n",
    "\n",
    "ML model's parameters can be split into two types.\n",
    "\n",
    "1. Parameters _internal_ to a model.\n",
    "\n",
    "$\\rightarrow$ These parameters can be estimated from a training data.\n",
    "\n",
    "2. Paremeters _external_ to model.\n",
    "\n",
    "$\\rightarrow$ Are called <span style = \"color:#42a5f5\"><b>hyperparameters</b></span>.\n",
    "<br>$\\rightarrow$ Cannot be estimated from data.\n",
    "\n",
    "Internal parameters are characterized by following properties\n",
    "\n",
    "- Necessary for making predictions.\n",
    "- Define the quality of a model on a given problem.\n",
    "- Can be learned from a training data.\n",
    "- Are usually part of a model.\n",
    "\n",
    "$\\rightarrow$ If model contains fixed number of internal parameters, model is called <span style = \"color:#42a5f5\"><b>parametric</b></span>.\n",
    "<br>$\\rightarrow$ <span style = \"color:#42a5f5\"><b>Non-parametric</b></span> otherwise.\n",
    "\n",
    "Examples of internal parameters are:\n",
    "\n",
    "- Weights of <span style = \"color:#42a5f5\"><b>artificial neural networks</b></span> (ANN).\n",
    "- Support vector values for SVM models.\n",
    "- Polynomial coefficients for linear regression or logistic regression.\n",
    "\n",
    "Hyperparameter have following characteristics:\n",
    "\n",
    "- Used to configure algorithms that estimate model parameters.\n",
    "- Usually defined by a practicioner.\n",
    "- Estimation is often based on using heuristics.\n",
    "- Specific to a modeling problem.\n",
    "\n",
    "$\\rightarrow$ Hyperparameters can be hard to estimate, and need micro-management.\n",
    "\n",
    "Some heuristics are\n",
    "\n",
    "- Rules of thumb,\n",
    "- copying values from similar projects,\n",
    "- grid search for hyperparameters.\n",
    "\n",
    "Examples of hyperparameters are:\n",
    "\n",
    "- C and sigma parameters used in a SVM algorithm for classification quality configuration.\n",
    "- Learning rate params used in neural network training process for configuring algorithm convergence.\n",
    "- The $k$ value used in KNN algorithm for configuring a number of neighbours.\n",
    "\n",
    "## Model parameter estimation\n",
    "\n",
    "Usually uses some optimiazation algorithm, and are used via\n",
    "algorithms based on the optimization of a loss function.\n",
    "\n",
    "$\\rightarrow$ Function that evaluates how well model predicts on the data is called <span style = \"color:#42a5f5\"><b>loss function</b></span>.\n",
    "\n",
    "If predicions are very different from target outputs, the loss\n",
    "function will return a value that is interpreted as a bad one.\n",
    "\n",
    "$\\rightarrow$ Is usually a large number.\n",
    "<br>$\\rightarrow$ In such a way, a loss function penalizes an optimization algorithm when it moves in the wrong direction.\n",
    "<br>$\\rightarrow$ Hence the general idea is to _minimize the value of loss function to reduce penalties_.\n",
    "<br>$\\rightarrow$ No universal loss function for optimization algorithms.\n",
    "\n",
    "Different factors deterine how to choose a loss function. Examples\n",
    "of such functions are:\n",
    "\n",
    "- Specifics of a given problem,\n",
    "- ease of calculating derivates, and\n",
    "- percentage of outliers in the dataset.\n",
    "\n",
    "Term <span style = \"color:#42a5f5\"><b>optimizer</b></span> is used\n",
    "to define an algorithm that connects a loss function and a\n",
    "technique for updating model parameters in response to the values\n",
    "of the loss funcion.\n",
    "\n",
    "Some optimizer examples:\n",
    "\n",
    "- <span style = \"color:#42a5f5\"><b>Gradient descent</b></span>,\n",
    "- <span style = \"color:#42a5f5\"><b>adagrad</b></span>,\n",
    "- <span style = \"color:#42a5f5\"><b>RMSProp</b></span>,\n",
    "- <span style = \"color:#42a5f5\"><b>Adam</b></span>,\n",
    "- and others.\n",
    "\n",
    "$\\rightarrow$ Developign optimizers is an active area of research.\n",
    "\n",
    "## An overview of linear algebra\n",
    "\n",
    "Development of GPUs furthered development of ML. Example libraries\n",
    "used for GPU computing are `Cuda`, `OpenCL`, and `cuBLAS`.\n",
    "Processor architectures have specialized instruction sets for\n",
    "vectorized computing; `AVx`, `SSE`, and `MMx`.\n",
    "\n",
    "$\\rightarrow$ Called _single instructions multiple data_ (SIMD) inst. sets.\n",
    "\n",
    "Also there's linear algebra libraries such as `Eigen`, `xtensor`,\n",
    "`VienaCL`, and others.\n",
    "\n",
    "## Learning the concepts of linear algebra\n",
    "\n",
    "Linear algebra studies objects of a linear nature: Vector (or\n",
    "linear) spaces, linear representatioins, and systems of linear\n",
    "equations. Main ideas of linear algebra are determinants, matrices,\n",
    "conjuation, and tensor calculus.\n",
    "\n",
    "## Basic linear algebra operations\n",
    "\n",
    "#### Element-wise operations\n",
    "\n",
    "Performed in an element-wise manner on vectors, matrices, or\n",
    "tensors of the same size.\n",
    "\n",
    "#### Dot product\n",
    "\n",
    "Deals with two equal-length series of numbers and returns a single\n",
    "number. A dot product example is:\n",
    "\n",
    "$$\n",
    "    \\mathbf{C}_{i,j} = \\sum_{k = 1}^m A_{i,k}B_{k,}\n",
    "$$\n",
    "\n",
    "where $i = 1, \\ldots, n$ and $j = 1, \\ldots, p$.\n",
    "\n",
    "#### Transposing\n",
    "\n",
    "$$\n",
    "    ( \\mathbf{A} )^\\top_{i,j} = \\mathbf{A}_{j,i}\n",
    "$$\n",
    "\n",
    "#### Norm\n",
    "\n",
    "This operation results in a size of a vector;\n",
    "\n",
    "$$\n",
    "    ||x||_p = \\left( \\sum_i |x_i|^p \\right)^{ \\frac{1}{p}  }\n",
    "$$\n",
    "\n",
    "Generic name is $L^p$ norm for $p \\in \\mathbb{R}, p \\geq <$.\n",
    "\n",
    "$\\rightarrow$ When $p = 2$, the norm is known as _Euclidean norm_.\n",
    "\n",
    "Another well known norm is the <span style =\n",
    "\"color:#42a5f5\"><b>squared $L^2$ norm</b></span>. Equation for the\n",
    "norm is $x^\\top x$.\n",
    "\n",
    "$\\rightarrow$ The squared $L^2$ norm is more suitable for mathematical and computational operations than the $L^2$ norm.\n",
    "\n",
    "Each partial derivative of squared $L^2$ norm depends only on the\n",
    "corresponding element of $x$, whereas the partial derivatives of\n",
    "the $L^2$ norm depends on the entire vector.\n",
    "\n",
    "$\\rightarrow$ This property has a vital role in optimization algorithms.\n",
    "\n",
    "#### Inversion\n",
    "\n",
    "...\n",
    "\n",
    "Matrices and tensors can be used to define training datasets for\n",
    "training, etc. Element-wise operations can be used to manipulate\n",
    "dataset, such as scaling. Linear algebra operations allows also for\n",
    "appplying many different transformations on dataset. Dot-product\n",
    "can be used to update model parameters etc.\n",
    "\n",
    "## Tensor representation in computing\n",
    "\n",
    "Tensors can be represented in computer's memory in many different\n",
    "ways. For example as a linear array.\n",
    "\n",
    "$\\rightarrow$ Either row-major or column-major ordering.\n",
    "<br>$\\rightarrow$ Data layouts have a significant impact on computational performance.\n",
    "<br>$\\rightarrow$ Cache misses etc.\n",
    "<br>$\\rightarrow$ Contiguous data allows for use of SIMD-operations.\n",
    "\n",
    "## Linear algebra API samples\n",
    "\n",
    "### Eigen\n",
    "\n",
    "`Eigen` is a general-purpose linear algebra C++ library. In\n",
    "`Eigen`, all matrices and vectors are objects of the `Matrix`\n",
    "template class.\n",
    "\n",
    "$\\rightarrow$ Tensor are not present in the official API, but exist as a submodules.\n",
    "\n",
    "Define a type for a matrix with known dimensions and floating-point data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e85878",
   "metadata": {},
   "outputs": [],
   "source": [
    "typedef Eigen::Matrix<float, 3, 3> MyMatrix33f;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bafa91a",
   "metadata": {},
   "source": [
    "and a vector by typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d82361",
   "metadata": {},
   "outputs": [],
   "source": [
    "typedef Eigen::Matrix<float, 3, 1> MyVector3f;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c30c1",
   "metadata": {},
   "source": [
    "`Eigen` already has many predefined types for vector and matrix\n",
    "objects, `Eigen::Matrix3f` (floating-point $3 \\times 3$ matrix\n",
    "type) or `Eigen::RowVector2f` (floating-point $1 \\times 2$ matrix).\n",
    "\n",
    "$\\rightarrow$ `Eigen` is not limited to static sized matrices.\n",
    "\n",
    "Run-time defined matrix can be defined as,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a30b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "typedef Eigen::Matrix<double, Eigen::Dynamic, Eigen::Dynamic> MyMatrix;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e90d795",
   "metadata": {},
   "source": [
    "The above type definitions can now be used as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2866e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyMatrix33f a;\n",
    "MyVector3f v;\n",
    "MyMatrix m(10,15);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8150c59",
   "metadata": {},
   "source": [
    "for example. The variables `a`, `v`, and `m` can be defined as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7eaeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = MyMatrix33f::Zero();\n",
    "a = MyMatrix33f::Identity();\n",
    "v = MyVector3f::Random();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2fd481",
   "metadata": {},
   "source": [
    "Matrices can be seeded with values by using the <span style =\n",
    "\"color:#42a5f5\"><b>comma-initializer</b></span> syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687138f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a << 1, 2, 3,\n",
    "     4, 5, 6,\n",
    "     7, 8, 9;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329615c8",
   "metadata": {},
   "source": [
    "so the matrix `a` would have the values\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    1 & 2 & 3 \\\\\n",
    "    4 & 5 & 6 \\\\\n",
    "    7 & 8 & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "as a result.\n",
    "\n",
    "Direc element access in `Eigen` happens with the `()` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dd796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a(0,0) = 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33108a2a",
   "metadata": {},
   "source": [
    "`Eigen`'s `Map` type can be used to wrap an existent C++ array or\n",
    "vector in the `Matrix` type object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915fc703",
   "metadata": {},
   "outputs": [],
   "source": [
    "int data[] = {1, 2, 3, 4};\n",
    "Eigen::Map<Eigen::RowVectorxi> v (data, 4);\n",
    "std::vector<float> data = {1, 2, 3, 4, 5, 6, 7, 8, 9};\n",
    "Eigen::Map<MyMatrix33f> a(data.data());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1eec37",
   "metadata": {},
   "source": [
    "Initialized matrices can be used in mathematical operations. For\n",
    "example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2486cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace Eigen;\n",
    "\n",
    "auto a = Matrix2d::Random();\n",
    "auto b = Matrix2d::Random();\n",
    "auto result = a + b;\n",
    "\n",
    "result = a.array() * b.array(); // element wise multiplication\n",
    "result = a.array() / b.array();\n",
    "a += b;\n",
    "result = a * b; // matrix multiplication\n",
    "a = b.array() * 4;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12aff8c",
   "metadata": {},
   "source": [
    "In `Eigen`, arithmetic operators such as `operator+` do not perform\n",
    "any computation by themselves. These operations return a <span\n",
    "style = \"color:#42a5f5\"><b>expression object</b></span>, which\n",
    "describe what computation to perform.\n",
    "\n",
    "Sometimes some computation is needed to be done only part of a\n",
    "block. `Eigen` allows this with the `block` method, which needs\n",
    "four parameters:\n",
    "\n",
    "- `i` and `j` that define a starting point, and\n",
    "- `p` and `q` that define the size of a block.\n",
    "\n",
    "Example using the `block` methods is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86dd5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eigen::Matrixxf m(4,4);\n",
    "Eigen::Matrix2f b = m.block(1,1,2,2);\n",
    "m.block(1,1,2,2) *= 4;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879dfd73",
   "metadata": {},
   "source": [
    "As many other linear algebra libraries, `Eigen` too supports <span\n",
    "style = \"color:#42a5f5\"><b>broadcasting</b></span> with the\n",
    "`colwise` and `rowwise` methods. For example, the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eigen::Matrixxf mat(2,4);\n",
    "Eigen::Vectorxf v(2); // column vector\n",
    "mat.colwise() += v;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18330122",
   "metadata": {},
   "source": [
    "is equivalent to result\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        1 & 2 & 3 \\\\\n",
    "        4 & 5 & 6\n",
    "    \\end{bmatrix}.\\operatorname{colwise()}\n",
    "    \\ + \\ \n",
    "    \\begin{bmatrix}\n",
    "        0 \\\\ 2\n",
    "    \\end{bmatrix}\n",
    "    \\ = \\ \n",
    "    \\begin{bmatrix}\n",
    "        1 & 2 & 3 \\\\\n",
    "        \\mathbf{5} & \\mathbf{6} & \\mathbf{7}\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "### Using `xtensor`\n",
    "\n",
    "The `xtensor` library is a C++ library for numerical analysis with\n",
    "multidimensional array expressions. Containers of `xtensor` are\n",
    "isnpired by NumPy, the Python array programming library.jo\n",
    "\n",
    "The `xtensor`'s `xarray` type is a dynamically sized\n",
    "multidimensional array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a15b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "std::vector<size_t> shape = { 3, 2, 4 };\n",
    "xt::array<double, xt::layout_type::row_major> a (shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e57726",
   "metadata": {},
   "source": [
    "The `xtensor` type is a multidimensional array whose dimensions are\n",
    "fixed at _compilation time_. Exact dimension values can be\n",
    "configured in the initialization step;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8457cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "std::array<size_t, 3> shape = { 3, 2, 4 };\n",
    "xt::tensor<double, 3> a(shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f2f718",
   "metadata": {},
   "source": [
    "The `xtensor_fixed` type is a multidimensional array with a\n",
    "dimension shape fixed at compile time, as shown in the following\n",
    "code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt::tensor_fixed<double, xt::shape<3, 2, 4>> a;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf914f",
   "metadata": {},
   "source": [
    "The `xtensor` library also implements arithmetic operators with\n",
    "expression template techniques such as `Eigen`.\n",
    "\n",
    "$\\rightarrow$ The computation happens lazily, and the actual result is calculated when the whole expression is evaluated.\n",
    "<br>$\\rightarrow$ Container definitions are also expressions.\n",
    "\n",
    "Different kinds of container initialization in the `xtensor` library.\n",
    "\n",
    "Example:\n",
    "\n",
    "- `xtensor` array initialized with _initializer lists_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e979a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt::xarray <double> arr1 {\n",
    "    {1.0, 2.0, 3.0},\n",
    "    {2.0, 5.0, 7.0},\n",
    "    {2.0, 5.0, 7.0}\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f964256",
   "metadata": {},
   "source": [
    "The `xtensor` library has builder functions for special tensor\n",
    "types. Examples;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be87a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "std::vector <uint64_t> shape = {2, 2};\n",
    "xt::ones(shape);\n",
    "xt::zero(shape);\n",
    "xt::eye(shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22647ff1",
   "metadata": {},
   "source": [
    "C++ arrays can be mapped to `xtensor` container with the\n",
    "`xt::adapt` function. The function `xt::adapt` return the object\n",
    "that uses the memory and values from the underlying object;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa57af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "std::vector<float> data {1, 2, 3, 4};\n",
    "std::vector<size_t> shape {2, 2};\n",
    "auto data_x = xt::adapt(data, shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7819ddcf",
   "metadata": {},
   "source": [
    "Direct access in `xtensor` container is done with the `operator()`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ac40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "std::vector<size_t> shape {3, 2, 4};\n",
    "xt::array<float> a = xt::ones<float>(shape);\n",
    "a(2,1,3) = 3.14f;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e57770",
   "metadata": {},
   "source": [
    "Partial access to the `xtensor` containers is done with `xt::view`\n",
    "function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f1e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt::array<int> a {\n",
    "    { 1,  2,  3,  4},\n",
    "    { 5,  6,  7,  8},\n",
    "    { 9, 10, 11, 12},\n",
    "    {12, 14, 15, 16}\n",
    "};\n",
    "\n",
    "auto b = xt::view(a, xt::range(1, 3), xt::range(1, 3));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f47d68",
   "metadata": {},
   "source": [
    "This results in a matrix view;\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        1  & 2  & 3  & 4  \\\\\n",
    "        5  & 6  & 7  & 8  \\\\\n",
    "        9  & 10 & 11 & 12 \\\\\n",
    "        13 & 14 & 15 & 16\n",
    "    \\end{bmatrix}\n",
    "    \\rightarrow\n",
    "    \\begin{bmatrix}\n",
    "        6 &  7 \\\\\n",
    "       10 & 11\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\rightarrow$ `xtensor` implements automatic broadcasting.\n",
    "<br>$\\rightarrow$ With two matrices of different sizes, `xtensor` broadcasts smaller dimension across the leading dimension of the other array.\n",
    "<br>$\\rightarrow$ Allows adding different sized matrices;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108e9ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto m = xt::random::rand<double>({2,2});\n",
    "auto v = xt::random::rand<double>({2,1});\n",
    "auto c = m + v;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ba3c5",
   "metadata": {},
   "source": [
    "### Using `Shark-ML`\n",
    "\n",
    "Four container class for representing matrices and vectors in the\n",
    "`Shark-ML` library.\n",
    "\n",
    "$\\rightarrow$ Linear algebra functionality is declared in the `remora` namespace.\n",
    "<br>$\\rightarrow$ Other functionality is in the `shark` namespace.\n",
    "\n",
    "Example of a container class in `Shark-ML` library. The following\n",
    "code creates a vector of size `100` filled with `1.0`.\n",
    "\n",
    "$$\n",
    "    remora::vector<double> b(100, 1.0);\n",
    "$$\n",
    "\n",
    "The `compressed_vector` type is a sparse array storing values in a\n",
    "compressed format.\n",
    "\n",
    "The `matrix` type is a dynamically sized dense matrix:\n",
    "\n",
    "$$\n",
    "    remora::matrix<double> C(2, 2);\n",
    "$$\n",
    "\n",
    "The `compressed_matrix` type is a sparse matrix storing values in a\n",
    "compressed format.\n",
    "\n",
    "$\\rightarrow$ Two ways to initialize a matrix\n",
    "\n",
    "1. Initializer list:\n",
    "\n",
    "$$\n",
    "    remora::matrix<float> m_ones {{1,1},{1,1}}; // 2 x 2 matrix\n",
    "$$\n",
    "\n",
    "2. Wrap existing C++ array into a container:\n",
    "\n",
    "$$\n",
    "    float data[] = {1, 2, 3, 4};\n",
    "    remora::matrix<float> m(data, 2, 2);\n",
    "    remora::vector<float> v(data, 4);\n",
    "$$\n",
    "\n",
    "$\\rightarrow$ Also there's ofcourse the possibility of using direct access with the `operator()`.\n",
    "\n",
    "`Shark-ML` allows for linear algebra.\n",
    "\n",
    "Partially accessing data in `Shark-ML` containers can be done with:\n",
    "\n",
    "- `subrange(x,i,j)`: Sub-vector of $x$ elements $\\langle x_i, \\ldots, x_{j-1} \\rangle$.\n",
    "- `subrange(A,i,j,k,l)`: Sub-matrix $A$ wiht elements $i, \\ldots, j-1$ and $k, \\ldots, l - 1$.\n",
    "- `row(A,k)`: `k`th column of `A` as a vector proxy.\n",
    "- `column(A,k)`: ...\n",
    "- `rows(A,k,l)`: ...\n",
    "- `columns(A,k,l)`: ...\n",
    "\n",
    "There's no broadcasting implementation in the `Shark-ML` library,\n",
    "but the library allows for reduction operations to be performed\n",
    "independently on matrix rows or columns respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f771e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "remora::matrix<float> m{{1,2,3,4}, {5,6,7,8}};\n",
    "auto cols = remora:as_columns(m);\n",
    "remora::sum(cols);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef6a0e",
   "metadata": {},
   "source": [
    "How to add the same vector to each of the matrix columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba737b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "remora::vector<float> v{10,10};\n",
    "// Update matrix rows\n",
    "for (size_t i = 0; i < m.size2(); ++i)\n",
    "    remora::column(m,i) += v;"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md,ipynb",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown"
   }
  },
  "kernelspec": {
   "display_name": "ROOT C++",
   "language": "c++",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".C",
   "mimetype": " text/x-c++src",
   "name": "c++"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
